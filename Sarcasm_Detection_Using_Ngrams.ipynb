{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sarcasm_Discrete_Final.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-gasior/Sarcasm-Detection/blob/master/Sarcasm_Detection_Using_Ngrams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0i1Dxg2xEkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Andrew Gasiorowski"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eqXFknwLN9n",
        "colab_type": "code",
        "outputId": "a6c020ca-6dac-41f6-ae28-b8e4c6e55afb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7gwhrMAMCLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re as _re\n",
        "import pandas as _pd\n",
        "import numpy as np\n",
        "import json as _js\n",
        "from math import log as _log\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from collections import Counter\n",
        "import pickle\n",
        "from operator import itemgetter\n",
        "import matplotlib.pyplot as _plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvRWZmswMC5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_pickles():\n",
        "  pickle_in = open(\"gdrive/My Drive/Data/pickles/reddit_training_x.pickle\",\"rb\")\n",
        "  x_train = pickle.load(pickle_in)\n",
        "  \n",
        "  pickle_in = open(\"gdrive/My Drive/Data/pickles/reddit_training_y.pickle\",\"rb\")\n",
        "  y_train = pickle.load(pickle_in)\n",
        "  \n",
        "  pickle_in = open(\"gdrive/My Drive/Data/pickles/reddit_testing_x.pickle\",\"rb\")\n",
        "  x_test = pickle.load(pickle_in)\n",
        "  \n",
        "  pickle_in = open(\"gdrive/My Drive/Data/pickles/reddit_testing_y.pickle\",\"rb\")\n",
        "  y_test = pickle.load(pickle_in)\n",
        "  \n",
        "  return x_train, y_train, x_test, y_test "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tssT4K6nMas1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _extract_uni_grams(x):\n",
        "  new_list = []\n",
        "  for idx,sentence in enumerate(x):\n",
        "    uni_gram = _re.findall(r'(?=\\b(\\w+)\\b)', sentence)\n",
        "    new_list.append(uni_gram)\n",
        "  return _pd.Series(new_list)\n",
        "\n",
        "\n",
        "def _extract_bi_grams(x):\n",
        "  new_list = []\n",
        "  for idx,sentence in enumerate(x):\n",
        "    bi_gram = _re.findall(r'(?=\\b(\\w+)\\b\\S* \\b(\\w+)\\b)', sentence)\n",
        "    new_list.append(bi_gram)\n",
        "  return _pd.Series(new_list)\n",
        "\n",
        "\n",
        "def _extract_tri_grams(x):\n",
        "  new_list = []\n",
        "  for idx,sentence in enumerate(x):\n",
        "    tri_gram = _re.findall(r'(?=\\b(\\w+)\\b\\S* \\b(\\w+)\\b\\S* \\b(\\w+)\\b)', sentence)\n",
        "    new_list.append(tri_gram)\n",
        "  return _pd.Series(new_list)\n",
        "  \n",
        "\n",
        "def _extract_n_gram(gram, x_train_whole_pre, this_size):\n",
        "  if gram == 1:\n",
        "      #do unigrams\n",
        "      x_train_whole = _extract_uni_grams(x_train_whole_pre.iloc[0:this_size])\n",
        "  if gram == 2:\n",
        "      #do bigrams\n",
        "      x_train_whole = _extract_bi_grams(x_train_whole_pre.iloc[0:this_size])\n",
        "  if gram == 3:\n",
        "    #do trigrams\n",
        "    x_train_whole = _extract_tri_grams(x_train_whole_pre.iloc[0:this_size])\n",
        "  return x_train_whole"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYfmeGvuMdQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Takes training corpus and the indexes for a fold. \n",
        "#Returns (pd.series) the samples for the fold.\n",
        "def separate_fold(x_train_whole, y_train_whole, train_idx, test_idx):\n",
        "    x_train, x_test = x_train_whole.iloc[train_idx], x_train_whole.iloc[test_idx]\n",
        "    y_train, y_test = y_train_whole.iloc[train_idx], y_train_whole.iloc[test_idx]\n",
        "    #Split train and test\n",
        "    x_train.reset_index(drop=True, inplace=True)\n",
        "    x_test.reset_index(drop=True, inplace=True)\n",
        "    y_train.reset_index(drop=True, inplace=True)\n",
        "    y_test.reset_index(drop=True, inplace=True)\n",
        "    #reset indexes to 0:(m-1)\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "#Takes (pd.series) training samples and their class\n",
        "#Returns (dict) bag of words for sarcasm and bow for not sarcasm\n",
        "def _make_bag_of_words(training_samples, training_samples_class):\n",
        "    unique_grams = Counter()\n",
        "    is_sarc_counts = Counter()\n",
        "    not_sarc_counts = Counter()\n",
        "    #thes dicts hold the counts of bi_grams \n",
        "    for idx,sentence in enumerate(training_samples):\n",
        "        #for every training sentence in this fold\n",
        "        for bg in sentence:\n",
        "            #for every n-gram    \n",
        "            unique_grams[bg] += 1\n",
        "            if training_samples_class.iloc[idx] == 1:\n",
        "                #if the sentence is sarcastic\n",
        "                is_sarc_counts[bg] += 1\n",
        "                #incrament that we've seen this n_gram as sarcastic\n",
        "            else:\n",
        "                #if sentence is not sarcastic\n",
        "                not_sarc_counts[bg] += 1\n",
        "                #incrament that we've seen this n_gram as not sarcastic\n",
        "    return unique_grams, is_sarc_counts, not_sarc_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "froWqJX0NP12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _naive_bayes(unique_bigrams, is_sarc_counts, not_sarc_counts, num_training_samples, num_sarc_training_samples, x_test, alpha):\n",
        "    num_unique_bg = len(unique_bigrams)\n",
        "    #total number of unique bigrams\n",
        "    num_sarc_bg = len(is_sarc_counts)\n",
        "    #number of sarcastic bigrams\n",
        "    num_not_sarc_bg = len(not_sarc_counts)\n",
        "    #number of not sarcastic bigrams\n",
        "    num_samples = num_training_samples\n",
        "    num_sarc_samples = num_sarc_training_samples\n",
        "    num_not_sarc_samples = num_samples - num_sarc_samples\n",
        "    #just some precomputations..\n",
        "    prior_prob_sarc = num_sarc_samples/num_samples\n",
        "    prior_prob_not_sarc = num_not_sarc_samples/num_samples\n",
        "    #the above lines compute the prior probabilites for both classes (sarc/not sarc)\n",
        "    y_predict_test = []\n",
        "    #holds the predicted class label\n",
        "    for idx,sentence in enumerate(x_test):\n",
        "        #for every sample\n",
        "        prob_sarc = _log(prior_prob_sarc, 10)\n",
        "        prob_not_sarc = _log(prior_prob_not_sarc, 10)\n",
        "        for bg in sentence:\n",
        "            prob_sarc = prob_sarc + _log((is_sarc_counts[bg] + alpha)/(num_sarc_bg + alpha*num_unique_bg), 10) \n",
        "            prob_not_sarc = prob_not_sarc + _log((not_sarc_counts[bg] + alpha)/(num_not_sarc_bg + alpha*num_unique_bg), 10)\n",
        "        #after computing all bi_grams for this sentence\n",
        "        if prob_sarc > prob_not_sarc:\n",
        "            #if sarcastic is more likely than not sarcastic\n",
        "            y_predict_test.append(1)\n",
        "        else:\n",
        "            #else predict not sarcastic\n",
        "            y_predict_test.append(0)\n",
        "    return y_predict_test\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "def _calculate_accuracy(y_actual, y_predict):\n",
        "    num_right = 0\n",
        "    total = len(y_actual)\n",
        "    for idx, prediction in enumerate(y_predict):\n",
        "        if prediction == y_actual.iloc[idx]:\n",
        "            num_right += 1\n",
        "    return ((num_right / total) * 100)\n",
        "  \n",
        "\n",
        "def _test_testing_data(x_train_whole, y_train_whole, x_test_whole, y_test_whole, alpha):\n",
        "  corpus_counts, sarcastic_counts, not_sarcastic_counts = _make_bag_of_words(x_train_whole, y_train_whole)\n",
        "  y_predict_whole = _naive_bayes(corpus_counts, sarcastic_counts, not_sarcastic_counts, len(x_train_whole), sum(y_train_whole), x_test_whole, alpha)\n",
        "  accuracy = _calculate_accuracy(y_test_whole, y_predict_whole)\n",
        "  return accuracy\n",
        "  \n",
        "  \n",
        "  \n",
        "#Takes list of tuples(alpha, acc) and range of alpha\n",
        "#returns list of tuples(alpha, avg_acc)\n",
        "#returns max alpha value\n",
        "def analyze_folds(acc_list, alpha_list):\n",
        "  avg_acc_list = []\n",
        "  for alpha in alpha_list:\n",
        "    this_avg = 0\n",
        "    for tup in acc_list:\n",
        "      if tup[0] == alpha:\n",
        "        this_avg += tup[1]\n",
        "    avg_acc_list.append((alpha,this_avg/5))\n",
        "  max_alpha = max(avg_acc_list,key=itemgetter(1))[0]\n",
        "  return avg_acc_list, max_alpha\n",
        "\n",
        "\n",
        "#takes avg_acc_list\n",
        "#saves chart to file\n",
        "def save_alpha_plot_to_file(data, gram_str, size_str, alpha):\n",
        "  filename = 'gdrive/My Drive/Data/data_out/alpha_train_' + gram_str + '_' + size_str + '.png'\n",
        "  _title = size_str + \" \" + gram_str + \" Alpha: \" + str(alpha)\n",
        "  _plt.scatter(*zip(*data))\n",
        "  _plt.title(_title)\n",
        "  _plt.xlabel(\"Alpha\")\n",
        "  _plt.ylabel(\"Accuracy\")\n",
        "  _plt.savefig(filename ,bbox_inches='tight')\n",
        "  _plt.clf()\n",
        "  \n",
        "def save_size_plot_to_file(data, gram_str, size_str):\n",
        "  filename = 'gdrive/My Drive/Data/data_out/SIZE_' + 'test' + '_' + gram_str + '.png'\n",
        "  _title = 'Test Accuracy by Size: ' + gram_str\n",
        "  _plt.scatter(*zip(*data))\n",
        "  _plt.title(_title)\n",
        "  _plt.xlabel(\"Number of Training Samples\")\n",
        "  _plt.ylabel(\"Accuracy\")\n",
        "  _plt.savefig(filename ,bbox_inches='tight')\n",
        "  _plt.clf()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr4yUnJWPIte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_size(x_train_whole, y_train_whole, x_test_whole, y_test_whole, gram_str, size_str):\n",
        "  alpha_list = range(1,50)\n",
        "  skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "  fold_num = 0\n",
        "  alpha_accuracy_list = []\n",
        "  #list of lists. sublists are accuracies for one fold by alpha value\n",
        "  for train_idx, test_idx in skf.split(x_train_whole, y_train_whole):\n",
        "      #for every fold\n",
        "      #print('Fold', str(fold_num+1))\n",
        "      x_train, y_train, x_test, y_test = separate_fold(x_train_whole, y_train_whole, train_idx, test_idx)\n",
        "      #separate training data for this fold\n",
        "      corpus_counts, sarcastic_counts, not_sarcastic_counts = _make_bag_of_words(x_train, y_train)\n",
        "      #make bag of words for this fold\n",
        "      for alpha in alpha_list:\n",
        "          y_predict = _naive_bayes(corpus_counts, sarcastic_counts, not_sarcastic_counts, len(x_train), sum(y_train), x_test, alpha)\n",
        "          #test training fold\n",
        "          accuracy = _calculate_accuracy(y_test, y_predict)\n",
        "          #print('\\talpha:',str(round(alpha, 1)),'\\taccuracy:',str(round(accuracy, 2)))\n",
        "          alpha_accuracy_list.append((alpha, accuracy))\n",
        "      fold_num += 1\n",
        "  avg_acc_list, optimal_alpha = analyze_folds(alpha_accuracy_list, alpha_list)\n",
        "  save_alpha_plot_to_file(avg_acc_list, gram_str, size_str, optimal_alpha)\n",
        "  test_acc = _test_testing_data(x_train_whole, y_train_whole, x_test_whole, y_test_whole, optimal_alpha)\n",
        "  return test_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMaQGUDLfXjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read in data and extract n-grams\n",
        "x_train_whole_pre, y_train_whole_pre, x_test_whole_pre, y_test_whole =  _get_pickles()\n",
        "\n",
        "\n",
        "sizes_list = [10000, 20000, 40000, 80000, 160000, 320000, 640000, len(x_train_whole_pre)]\n",
        "grams_list = [1,2,3]\n",
        "gram_str = ['unigram','bigram','trigram']\n",
        "size_str = ['10k','20k','40k','80k','160k','320k','640k','810k']\n",
        "\n",
        "\n",
        "for idx_g, gram in enumerate(grams_list):\n",
        "    accuracy_by_size = []\n",
        "    for idx_s, this_size in enumerate(sizes_list):\n",
        "        print('currently on size ', size_str[idx_s], ' of ', gram_str[idx_g])\n",
        "        x_train_whole = _extract_n_gram(gram, x_train_whole_pre, this_size)\n",
        "        y_train_whole = y_train_whole_pre.iloc[0:this_size]\n",
        "        x_test_whole = _extract_n_gram(gram, x_test_whole_pre, this_size)\n",
        "        this_accuracy = run_size(x_train_whole, y_train_whole, x_test_whole, y_test_whole, gram_str[idx_g], size_str[idx_s])\n",
        "        accuracy_by_size.append( (this_size, this_accuracy) )\n",
        "    save_size_plot_to_file(accuracy_by_size, gram_str[idx_g], size_str[idx_s])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}